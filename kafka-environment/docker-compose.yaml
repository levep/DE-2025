version: '3.2'
services:

    # minio:
    #     container_name: minio
    #     image: minio/minio:RELEASE.2022-11-08T05-27-07Z
    #     command: server /data --console-address ":9001"
    #     ports:
    #         - "9001:9000"
    #         - "9002:9001"

    zookeeper:
        container_name: zookeeper
        image: wurstmeister/zookeeper:latest
        ports:
            - "2181:2181"

    course-kafka:
        container_name: course-kafka
        image: wurstmeister/kafka:2.13-2.8.1
        environment:
            KAFKA_ADVERTISED_HOST_NAME: course-kafka
            KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
        ports:
            - "9092:9092"
        depends_on:
            - zookeeper

    kafdrop:
        container_name: kafdrop
        image: obsidiandynamics/kafdrop:3.30.0
        ports:
            - "9003:9000"
        environment:
            - KAFKA_BROKERCONNECT=course-kafka:9092
        depends_on:
            - course-kafka

    # kafka-connect:
    #     container_name: kafka-connect
    #     image: confluentinc/cp-kafka-connect:7.3.2
    #     depends_on:
    #         - course-kafka
    #         - mongo
    #     ports:
    #         - "8083:8083"
    #     environment:
    #         CONNECT_BOOTSTRAP_SERVERS: course-kafka:9092
    #         CONNECT_REST_PORT: 8083
    #         CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
    #         CONNECT_GROUP_ID: kafka-connect-group
    #         CONNECT_CONFIG_STORAGE_TOPIC: connect-configs
    #         CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets
    #         CONNECT_STATUS_STORAGE_TOPIC: connect-status
    #         CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
    #         CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
    #         CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
    #         CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
    #         CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
    #         CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
    #         CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/plugins
    #     volumes:
    #         - ./mongo-kafka-connect:/plugins/mongo            

    python-app:
        image: python:3.11-slim
        container_name: python-kafka-container
        command: tail -f /dev/null  # Keeps the container alive
        volumes:
        - ./app:/app
        working_dir: /app
        tty: true
        stdin_open: true
        entrypoint: >
            sh -c "apt-get update &&
                apt-get install -y vim nano &&
                pip install --no-cache-dir kafka-python &&
                tail -f /dev/null"
            
    # mariadb:
    #     container_name: mariadb
    #     image: mariadb:10.6
    #     ports:
    #         - 3306:3306
    #     environment:
    #         MYSQL_ROOT_PASSWORD: admin
    #         MYSQL_USER: admin
    #         MYSQL_PASSWORD: admin
    #         MYSQL_DATABASE: metastore_db
    
    # hive-metastore:
    #     container_name: hive_metastore
    #     image: ofrir119/hive-metastore:1.0
    #     ports:
    #         - 9083:9083
    #     depends_on:
    #         - mariadb
        
    # dev_env:
    #     container_name: dev_env
    #     image: ofrir119/developer_env:spark340_ssh
    #     ports:
    #         - "22022:22"
    #         - "8888:8888"
    #         - "4040:4040"
    #         - "4041:4041"
    #         - "4042:4042"

    mongo:
        image: mongo:8.0
        container_name: mongo
        volumes:
        - ./data:/data
        working_dir: /data        
        ports:
            - "27017:27017"
            
    # nifi:
    #     image: nayacollege/nifi:1.0
    #     container_name: nifi
    #     ports:
    #         - "8080:8080"
            
    elasticsearch:
        container_name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.13.2
        ports:
            - "9200:9200"
        environment:
            discovery.type: single-node
            
    kibana:
        container_name: kibana
        image: docker.elastic.co/kibana/kibana:7.13.2
        ports:
            - "5601:5601"
        depends_on:
            - elasticsearch
            
    # postgres:
    #     container_name: postgres
    #     image: postgres:12
    #     environment:
    #         - POSTGRES_USER=postgres
    #         - POSTGRES_PASSWORD=postgres
    #         - POSTGRES_DB=airflow
    #         - POSTGRES_PORT=5432
    #     ports:
    #         - "5432:5432"

    # airflow-init:
    #     image: apache/airflow:2.0.0
    #     environment:
    #         - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    #         - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
    #         - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
    #         - AIRFLOW__CORE__LOAD_EXAMPLES=False
    #         - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    #     volumes:
    #         - ./dags:/opt/airflow/dags
    #         - ./airflow-data/logs:/opt/airflow/logs
    #         - ./airflow-data/plugins:/opt/airflow/plugins
    #         - ./airflow-data/airflow.cfg:/opt/airlfow/airflow.cfg
    #     depends_on:
    #         - postgres
    #     container_name: airflow_init
    #     entrypoint: /bin/bash
    #     command:
    #         - -c
    #         - airflow users list || ( airflow db init &&
    #             airflow users create
    #             --role Admin
    #             --username airflow
    #             --password airflow
    #             --email airflow@airflow.com
    #             --firstname airflow
    #             --lastname airflow )
    #     restart: on-failure

    # airflow-webserver:
    #     image: apache/airflow:2.0.0
    #     environment:
    #         - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    #         - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
    #         - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
    #         - AIRFLOW__CORE__LOAD_EXAMPLES=False
    #         - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    #     volumes:
    #         - ./dags:/opt/airflow/dags
    #         - ./airflow-data/logs:/opt/airflow/logs
    #         - ./airflow-data/plugins:/opt/airflow/plugins
    #         - ./airflow-data/airflow.cfg:/opt/airlfow/airflow.cfg
    #     depends_on:
    #         - postgres
    #     command: airflow webserver
    #     ports:
    #         - 8082:8080
    #     container_name: airflow_webserver
    #     restart: always

    # airflow-scheduler:
    #     image: apache/airflow:2.0.0
    #     environment:
    #         - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    #         - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow
    #         - AIRFLOW__CORE__FERNET_KEY=FB0o_zt4e3Ziq3LdUUO7F2Z95cvFFx16hU8jTeR1ASM=
    #         - AIRFLOW__CORE__LOAD_EXAMPLES=False
    #         - AIRFLOW__CORE__LOGGING_LEVEL=INFO
    #     volumes:
    #         - ./dags:/opt/airflow/dags
    #         - ./airflow-data/logs:/opt/airflow/logs
    #         - ./airflow-data/plugins:/opt/airflow/plugins
    #         - ./airflow-data/airflow.cfg:/opt/airlfow/airflow.cfg
    #     depends_on:
    #         - postgres
    #     command: airflow scheduler
    #     container_name: airflow_scheduler
    #     restart: always